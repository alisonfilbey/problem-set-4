---
title: "30538 Problem Set 5: Web Scraping "
author: "Alison Filbey and Claire Conzelmann"
date: "today"
format: 
  pdf:
    include-in-header: 
       text: |
         \usepackage{fvextra}
         \DefineVerbatimEnvironment{Highlighting}{Verbatim}{breaklines,commandchars=\\\{\}}
include-before-body:
  text: |
    \RecustomVerbatimEnvironment{verbatim}{Verbatim}{
      showspaces = false,
      showtabs = false,
      breaksymbolleft={},
      breaklines
    }
---

## Submission Steps (10 pts)
1. This problem set is a paired problem set.
2. Play paper, scissors, rock to determine who goes first. Call that person Partner 1.
• Partner 1 (name and cnet ID): Alison Filbey afilbey
• Partner 2 (name and cnet ID): Claire Conzelmann cconzelmann
3. Partner 1 will accept the ps5 and then share the link it creates with their partner. You
can only share it with one partner so you will not be able to change it after your partner
has accepted.
4. “This submission is our work alone and complies with the 30538 integrity policy.” Add
your initials to indicate your agreement: **AF** **CC**
5. “I have uploaded the names of anyone else other than my partner and I worked with on
the problem set here” (1 point)**AF** **CC**
6. Late coins used this pset: **__** Late coins left after submission: **__**
7. Knit your ps5.qmd to an PDF file to make ps5.pdf,
• The PDF should not be more than 25 pages. Use head() and re-size figures when
appropriate.
8. (Partner 1): push ps5.qmd and ps5.pdf to your github repo.
9. (Partner 1): submit ps5.pdf via Gradescope. Add your partner on Gradescope.
10. (Partner 1): tag your submission in Gradescop



## Step 1: Develop initial scraper and crawler

```{python}
#libraries
import pandas as pd
import altair as alt
import requests
from bs4 import BeautifulSoup
from urllib.parse import urljoin
import shapely
from shapely import Polygon, Point
import geopandas as gpd
import time
from datetime import datetime 
from concurrent.futures import ThreadPoolExecutor
```

1. (Partner 1) Scraping: Go to the first page of the HHS OIG’s “Enforcement Actions”
page and scrape and collect the following into a dataset:
• Title of the enforcement action
• Date
• Category (e.g, “Criminal and Civil Actions”)
• Link associated with the enforcement action
Collect your output into a tidy dataframe and print its head.

```{python}
url ='https://oig.hhs.gov/fraud/enforcement/'
response = requests.get(url)
soup = BeautifulSoup(response.text, 'lxml')

#Title 
h2 = soup.find_all('h2')
title = []
for h2 in h2:
   title.append(h2.get_text(strip=True))
title=title[2:22]

#Date
span = soup.find_all('span', class_='text-base-dark padding-right-105')
date = []
for span in span:
   date.append(span.get_text(strip=True))

#Classification
listed = soup.find_all('li', class_='display-inline-block usa-tag text-no-lowercase text-base-darkest bg-base-lightest margin-right-1')
classification = []
for element in listed:
    text = element.get_text(strip=True)
    classification.append(str(text))

#Link
a_tags = soup.find_all('a')
links = []
for a in a_tags:
    if 'href' in a.attrs and a['href']:
        link = a['href']
        links.append(link)  

links=links[74:94]
short_url = 'https://oig.hhs.gov'
links = [short_url + link for link in links]

enforcement_actions = {
    'Title': title,
    'Date': date,
    'Classification': classification,
    'URL': links
}

enforcement_actions = pd.DataFrame(enforcement_actions)
enforcement_actions.head()
```

2. Crawling: Then for each enforcement action, click the link and collect the name of the agency involved (e.g., for this link, it would be U.S. Attorney’s Office, Eastern District of Washington). Update your dataframe with the name of the agency and print its head

```{python}
agency = []
for crawl_url in enforcement_actions['URL']:
  response = requests.get(crawl_url)
  soup = BeautifulSoup(response.text, 'lxml')
  span = soup.find('span', string=lambda text: text and 'Agency:' in text)
  if span:
    agency_name = span.find_next_sibling(text=True).strip()
    agency.append(agency_name)
  else:
     None

enforcement_actions['Agency'] = agency
enforcement_actions.head()
```

# Step 2: Making the scraper dynamic

### 1. Turning the scraper into a function 

* a. Pseudo-Code (PARTNER 2)

1. check if year inputted is < 2013
  - if <2013, print "Invalid year." (use an if statement here)
2. Set base url (without the page info), base page=1, and empty lists to be appended with each iteration
3. Begin a while loop
  - while loop will continue looping through pages and exit once there are no more pages to scrape
4. Go through first page, extract soup and all relavant tags
5. Use for loop to extract each item in the find_all tag lists and append to empty lists created before
6. Check if date of given item is outside of the range inputted in the function
  - if it is, exit out of the function
7. Add 1 to page and repeat


* b. Create Dynamic Scraper (PARTNER 2)
I originally included the crawling in my for loop within the function. This was taking a really long time to run. I pasted my code into ChatGPT and asked how to make it run faster. It suggested I create a separate function for the crawling and use ThreadPoolExecutor.

```{python}
#function to crawl extracted links and get agency name
def fetch_agency_name(link):
    """
    Takes a link as an input and fetches the agency name from the given link
    """

    #get url to craawl
    short_url = "https://oig.hhs.gov"
    if not link.startswith("http"):
        full_link = short_url + link
    else:
        full_link = link

    response = requests.get(full_link)

    #crawl the link that is extracted on the main page and extract agency name
    soup_crawl = BeautifulSoup(response.text, "lxml")
    span_crawl = soup_crawl.find("span", 
      string=lambda text: text and "Agency:" in text)
    return span_crawl.find_next_sibling(string=True).strip() if span_crawl else ""

#main scraping function to scrape oig website through specified date
def dynamic_scraper(year, month):

  #create datetime object from year and month
  date_obj = datetime(year, month, 1)

  #first confirm year is in range
  if year < 2013:
    print("Invalid year. Please enter a year that is 2013 or later.")
    return 
  
  #set base url and first page
  base_url = "https://oig.hhs.gov/fraud/enforcement/"
  page = 1

  #set empty lists to store results
  title = []
  date = []
  classification = []
  links = []
  agency = []

  #set short url to append to url in loop
  short_url = 'https://oig.hhs.gov'

  #create soup object from given page
  while True:
    url = base_url + "?page=" + str(page)
    response = requests.get(url)
    soup = BeautifulSoup(response.content, 'html.parser')
    
    #get out of loop if there is no page to scrape
    if not soup:
      break
    
    #find all necessary tags on given page
    h2_tags = soup.find_all("h2")[2:]
    span_tags = soup.find_all("span", class_="text-base-dark padding-right-105")
    li_tags = soup.find_all("li", 
      class_="display-inline-block usa-tag text-no-lowercase text-base-darkest bg-base-lightest margin-right-1")
    a_tags = soup.find_all("a")[74:74+len(span_tags)]

    for i in range(len(span_tags)):
      #turn date string into datetime object
      full_date = datetime.strptime(span_tags[i].get_text(strip=True), "%B %d, %Y")
        
      #stop scraping if date is outside the range
      if full_date < date_obj:

        #fetch agency names before exiting the loop
        with ThreadPoolExecutor(max_workers=5) as executor:
          agency = list(executor.map(fetch_agency_name, links))

        #write dataframe
        df = pd.DataFrame({"Title": title, 
                            "Date": date,
                            "Classification": classification, 
                            "URL": links,
                            "Agency" : agency})
        
        #write csv
        file_name = "Data/enforcement_actions_" + str(year) + str(month) + ".csv"
        df.to_csv(file_name)

        #exit function
        return df

      #extract date and append to date column
      date.append(span_tags[i].get_text(strip=True))

      #extract data from other tags and append to respective columns
      title.append(h2_tags[i].get_text(strip=True))

      text = li_tags[i].get_text(strip=True)
      classification.append(str(text))

      if 'href' in a_tags[i].attrs and a_tags[i]['href']:
        link = a_tags[i]['href']
        link = short_url + link
        links.append(link)

    page += 1
      
    #delay
    time.sleep(1)

  #fetch agency names in parallel
  with ThreadPoolExecutor(max_workers=5) as executor:
    agency = list(executor.map(fetch_agency_name, links))

  #write dataframe
  df = pd.DataFrame({"Title": title, 
                    "Date": date,
                    "Classification": classification, 
                    "URL": links,
                    "Agency" : agency})
        
  #write csv
  file_name = "Data/enforcement_actions_" + str(year) + str(month) + ".csv"
  df.to_csv(file_name)

  return df
```

```{python}
#test dynamic scraper
enforcement_actions_012023 = dynamic_scraper(2023, 1)
```

```{python}
#get info on tested function
print(len(enforcement_actions_012023))

#convert date to datetime so we can calculate the minimum
enforcement_actions_012023["Date_obj"] = pd.to_datetime(
  enforcement_actions_012023["Date"], format="%B %d, %Y")

min_date = min(enforcement_actions_012023["Date_obj"])

enforcement_actions_012023.loc[enforcement_actions_012023["Date_obj"] == min_date]
```

There are 1,510 enforcement actions in the final dataframe. The earliest enforcement action scraped was from January 3, 2023, and the title, classification, agency, and url that was scraped for this enforcement action is listed above. 

* c. Test Partner's Code (PARTNER 1)

```{python}
enforcement_actions_012021 = dynamic_scraper(2021, 1)
```

## Step 3: Plot data based on scraped data

### 1. Plot the number of enforcement actions over time (PARTNER 2)

```{python}

```

### 2. Plot the number of enforcement actions categorized: (PARTNER 1)

* based on "Criminal and Civil Actions" vs. "State Enforcement Agencies"

```{python}

```

* based on five topics

```{python}

```

## Step 4: Create maps of enforcement activity

### 1. Map by State (PARTNER 1)

```{python}

```


### 2. Map by District (PARTNER 2)

```{python}

```

## Extra Credit

### 1. Merge zip code shapefile with population
```{python}
#import zipcode shapefile
zipcode_shapefile = "/Users/claireconzelmann/Documents/GitHub/Python2/problem-set-4-alison-and-claire/Data/gz_2010_us_860_00_500k.shp"
zipcodes_sp = gpd.read_file(zipcode_shapefile)

#import zipcode population file
zip_pop = pd.read_csv("/Users/claireconzelmann/Downloads/DECENNIALDHC2020.P1_2024-11-05T221849/DECENNIALDHC2020.P1-Data.csv")

#import district shapefile
district_shapefile = "/Users/claireconzelmann/Downloads/US Attorney Districts Shapefile simplified_20241105/geo_export_31250f05-1141-499d-9d2a-48d87472e3a4.shp"
districts_sp = gpd.read_file(district_shapefile)
```

```{python}
#create zipcode variable
zip_pop = zip_pop.drop(index=0).reset_index(drop=True)
zip_pop["ZCTA5"] = zip_pop["GEO_ID"].str[-5:].astype(int).astype(str)

#change zipcode to integer
zipcodes_sp["ZCTA5"] = zipcodes_sp["ZCTA5"].astype(int).astype(str)

#merge population counts to zipcode shapefile
zipcodes_sp = pd.merge(zipcodes_sp, zip_pop, on=["ZCTA5"], how="left")
```

### 2. Conduct spatial join
```{python}
#spatial join districts to zipcodes
zipcodes_to_districts = gpd.sjoin(zipcodes_sp, 
  districts_sp, how="inner", predicate="intersects")

#recast population as integer
zipcodes_to_districts["P1_001N"] = zipcodes_to_districts["P1_001N"].fillna(0).astype(int)

#aggregate zipcode population to districts
district_pops = zipcodes_to_districts.groupby(["district_n"])["P1_001N"].sum().reset_index()
```

### 3. Map the action ratio in each district
```{python}

```