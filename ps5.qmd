---
title: "30538 Problem Set 5: Web Scraping "
author: "Alison Filbey and Claire Conzelmann"
date: "today"
format: 
  pdf:
    include-in-header: 
       text: |
         \usepackage{fvextra}
         \DefineVerbatimEnvironment{Highlighting}{Verbatim}{breaklines,commandchars=\\\{\}}
include-before-body:
  text: |
    \RecustomVerbatimEnvironment{verbatim}{Verbatim}{
      showspaces = false,
      showtabs = false,
      breaksymbolleft={},
      breaklines
    }
---

## Submission Steps (10 pts)
1. This problem set is a paired problem set.
2. Play paper, scissors, rock to determine who goes first. Call that person Partner 1.
• Partner 1 (name and cnet ID): Alison Filbey afilbey
• Partner 2 (name and cnet ID): Claire Conzelmann cconzelmann
3. Partner 1 will accept the ps5 and then share the link it creates with their partner. You
can only share it with one partner so you will not be able to change it after your partner
has accepted.
4. “This submission is our work alone and complies with the 30538 integrity policy.” Add
your initials to indicate your agreement: **AF** **CC**
5. “I have uploaded the names of anyone else other than my partner and I worked with on
the problem set here” (1 point)**AF** **CC**
6. Late coins used this pset: **__** Late coins left after submission: **__**
7. Knit your ps5.qmd to an PDF file to make ps5.pdf,
• The PDF should not be more than 25 pages. Use head() and re-size figures when
appropriate.
8. (Partner 1): push ps5.qmd and ps5.pdf to your github repo.
9. (Partner 1): submit ps5.pdf via Gradescope. Add your partner on Gradescope.
10. (Partner 1): tag your submission in Gradescop



## Step 1: Develop initial scraper and crawler

```{python}
#libraries
import pandas as pd
import altair as alt
import requests
from bs4 import BeautifulSoup
from urllib.parse import urljoin
import shapely
from shapely import Polygon, Point
import geopandas as gpd
```

1. (Partner 1) Scraping: Go to the first page of the HHS OIG’s “Enforcement Actions”
page and scrape and collect the following into a dataset:
• Title of the enforcement action
• Date
• Category (e.g, “Criminal and Civil Actions”)
• Link associated with the enforcement action
Collect your output into a tidy dataframe and print its head.

```{python}
url ='https://oig.hhs.gov/fraud/enforcement/'
response = requests.get(url)
soup = BeautifulSoup(response.text, 'lxml')

#Title 
h2 = soup.find_all('h2')
title = []
for h2 in h2:
   title.append(h2.get_text(strip=True))
title=title[2:22]

#Date
span = soup.find_all('span', class_='text-base-dark padding-right-105')
date = []
for span in span:
   date.append(span.get_text(strip=True))

#Classification
listed = soup.find_all('li', class_='display-inline-block usa-tag text-no-lowercase text-base-darkest bg-base-lightest margin-right-1')
classification = []
for element in listed:
    text = element.get_text(strip=True)
    classification.append(str(text))

#Link
a_tags = soup.find_all('a')
links = []
for a in a_tags:
    if 'href' in a.attrs and a['href']:
        link = a['href']
        links.append(link)  

links=links[74:94]
short_url = 'https://oig.hhs.gov'
links = [ short_url + link for link in links]

enforcement_actions = {
    'Title': title,
    'Date': date,
    'Classification': classification,
    'URL': links
}

enforcement_actions = pd.DataFrame(enforcement_actions)
enforcement_actions.head()
```

2. Crawling: Then for each enforcement action, click the link and collect the name of the agency involved (e.g., for this link, it would be U.S. Attorney’s Office, Eastern District of Washington). Update your dataframe with the name of the agency and print its head

```{python}
agency = []
for crawl_url in enforcement_actions['URL']:
  response = requests.get(crawl_url)
  soup = BeautifulSoup(response.text, 'lxml')
  span = soup.find('span', string=lambda text: text and 'Agency:' in text)
  if span:
    agency_name = span.find_next_sibling(text=True).strip()
    agency.append(agency_name)
  else:
     None

enforcement_actions['Agency'] = agency
```

# Step 2: Making the scraper dynamic

### 1. Turning the scraper into a function 

* a. Pseudo-Code (PARTNER 2)


* b. Create Dynamic Scraper (PARTNER 2)

```{python}

```

* c. Test Partner's Code (PARTNER 1)

```{python}

```

## Step 3: Plot data based on scraped data

### 1. Plot the number of enforcement actions over time (PARTNER 2)

```{python}

```

### 2. Plot the number of enforcement actions categorized: (PARTNER 1)

* based on "Criminal and Civil Actions" vs. "State Enforcement Agencies"

```{python}

```

* based on five topics

```{python}

```

## Step 4: Create maps of enforcement activity

### 1. Map by State (PARTNER 1)

```{python}

```


### 2. Map by District (PARTNER 2)

```{python}

```

## Extra Credit

### 1. Merge zip code shapefile with population
```{python}
#import zipcode shapefile
zipcode_shapefile = "/Users/claireconzelmann/Documents/GitHub/Python2/problem-set-4-alison-and-claire/Data/gz_2010_us_860_00_500k.shp"
zipcodes_sp = gpd.read_file(zipcode_shapefile)

#import zipcode population file
zip_pop = pd.read_csv("/Users/claireconzelmann/Downloads/DECENNIALDHC2020.P1_2024-11-05T221849/DECENNIALDHC2020.P1-Data.csv")

#import district shapefile
district_shapefile = "/Users/claireconzelmann/Downloads/US Attorney Districts Shapefile simplified_20241105/geo_export_31250f05-1141-499d-9d2a-48d87472e3a4.shp"
districts_sp = gpd.read_file(district_shapefile)
```

```{python}
#create zipcode variable
zip_pop = zip_pop.drop(index=0).reset_index(drop=True)
zip_pop["ZCTA5"] = zip_pop["GEO_ID"].str[-5:].astype(int).astype(str)

#change zipcode to integer
zipcodes_sp["ZCTA5"] = zipcodes_sp["ZCTA5"].astype(int).astype(str)

#merge population counts to zipcode shapefile
zipcodes_sp = pd.merge(zipcodes_sp, zip_pop, on=["ZCTA5"], how="left")
```

### 2. Conduct spatial join
```{python}
#spatial join districts to zipcodes
zipcodes_to_districts = gpd.sjoin(zipcodes_sp, 
  districts_sp, how="inner", predicate="intersects")

#recast population as integer
zipcodes_to_districts["P1_001N"] = zipcodes_to_districts["P1_001N"].fillna(0).astype(int)

#aggregate zipcode population to districts
district_pops = zipcodes_to_districts.groupby(["district_n"])["P1_001N"].sum().reset_index()
```

### 3. Map the action ratio in each district
```{python}

```