---
title: "30538 Problem Set 5: Web Scraping "
author: "Alison Filbey"
date: "today"
format: 
  pdf:
    include-in-header: 
       text: |
         \usepackage{fvextra}
         \DefineVerbatimEnvironment{Highlighting}{Verbatim}{breaklines,commandchars=\\\{\}}
include-before-body:
  text: |
    \RecustomVerbatimEnvironment{verbatim}{Verbatim}{
      showspaces = false,
      showtabs = false,
      breaksymbolleft={},
      breaklines
    }
---

## Submission Steps (10 pts)
1. This problem set is a paired problem set.
2. Play paper, scissors, rock to determine who goes first. Call that person Partner 1.
• Partner 1 (name and cnet ID): afilbey
• Partner 2 (name and cnet ID):
3. Partner 1 will accept the ps5 and then share the link it creates with their partner. You
can only share it with one partner so you will not be able to change it after your partner
has accepted.
4. “This submission is our work alone and complies with the 30538 integrity policy.” Add
your initials to indicate your agreement: **AF** **CC**
5. “I have uploaded the names of anyone else other than my partner and I worked with on
the problem set here” (1 point)**AF** **CC**
6. Late coins used this pset: **__** Late coins left after submission: **__**
7. Knit your ps5.qmd to an PDF file to make ps5.pdf,
• The PDF should not be more than 25 pages. Use head() and re-size figures when
appropriate.
8. (Partner 1): push ps5.qmd and ps5.pdf to your github repo.
9. (Partner 1): submit ps5.pdf via Gradescope. Add your partner on Gradescope.
10. (Partner 1): tag your submission in Gradescop



## Step 1: Develop initial scraper and crawler

```{python}
#libraries
import pandas as pd
import altair as alt
import requests
from bs4 import BeautifulSoup
from urllib.parse import urljoin
```

1. (Partner 1) Scraping: Go to the first page of the HHS OIG’s “Enforcement Actions”
page and scrape and collect the following into a dataset:
• Title of the enforcement action
• Date
• Category (e.g, “Criminal and Civil Actions”)
• Link associated with the enforcement action
Collect your output into a tidy dataframe and print its head.

```{python}
url ='https://oig.hhs.gov/fraud/enforcement/'
response = requests.get(url)
soup = BeautifulSoup(response.text, 'lxml')

#Title 
h2 = soup.find_all('h2')
title = []
for h2 in h2:
   title.append(h2.get_text(strip=True))
title=title[2:22]

#Date
span = soup.find_all('span', class_='text-base-dark padding-right-105')
date = []
for span in span:
   date.append(span.get_text(strip=True))

#Classification
listed = soup.find_all('li', class_='display-inline-block usa-tag text-no-lowercase text-base-darkest bg-base-lightest margin-right-1')
classification = []
for element in listed:
    text = element.get_text(strip=True)
    classification.append(str(text))

#Link
a_tags = soup.find_all('a')
links = []
for a in a_tags:
    if 'href' in a.attrs and a['href']:
        link = a['href']
        links.append(link)  

links=links[74:94]
short_url = 'https://oig.hhs.gov'
links = [ short_url + link for link in links]

enforcement_actions = {
    'Title': title,
    'Date': date,
    'Classification': classification,
    'URL': links
}

enforcement_actions = pd.DataFrame(enforcement_actions)
enforcement_actions.head()
```

2. Crawling: Then for each enforcement action, click the link and collect the name of the agency involved (e.g., for this link, it would be U.S. Attorney’s Office, Eastern District of Washington). Update your dataframe with the name of the agency and print its head

```{python}
agency = []
for crawl_url in enforcement_actions['URL']:
  response = requests.get(crawl_url)
  soup = BeautifulSoup(response.text, 'lxml')
  span = soup.find('span', string=lambda text: text and 'Agency:' in text)
  if span:
    agency_name = span.find_next_sibling(text=True).strip()
    agency.append(agency_name)
  else:
     None

enforcement_actions['Agency'] = agency
```